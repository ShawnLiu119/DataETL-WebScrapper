{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgHdrt48GfoC6ZQnGI/D5M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShawnLiu119/DataETL-WebScrapper/blob/main/1.NLP_TextScrapeTokenCount_DataETL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Data ETL pipeline - Web Scrap**"
      ],
      "metadata": {
        "id": "c4KSbXN2LeKx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhrx_YX2LWlK",
        "outputId": "83f4ba13-9b20-47a9-9300-52bbe09015a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import requests #allow send HTTP requess in Python\n",
        "from bs4 import BeautifulSoup #scrape information from web pages\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter #Counter is an unordered collection where elements are stored as Dict keys and their count as dict value.\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyGfx8BELsSB",
        "outputId": "654aa516-ae32-4dc9-fa61-3d85e444c731"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WebScraper:\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "\n",
        "    def extract_article_text(self):\n",
        "        response = requests.get(self.url)\n",
        "        html_content = response.content\n",
        "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "        article_text = soup.get_text()\n",
        "        return article_text"
      ],
      "metadata": {
        "id": "_nBML_B1MgwM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, the WebScraper class provides a way to conveniently extract the main text content of an article from a given web page URL. By creating an instance of the WebScraper class and calling its extract_article_text method, we can retrieve the textual data of the article, which can then be further processed or analyzed as needed."
      ],
      "metadata": {
        "id": "Dc8bHShINyZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextProcessor:\n",
        "    def __init__(self, nltk_stopwords):\n",
        "        self.nltk_stopwords = nltk_stopwords\n",
        "\n",
        "    def tokenize_and_clean(self, text):\n",
        "        words = text.split()\n",
        "        filtered_words = [word.lower() for word in words if word.isalpha() and word.lower() not in self.nltk_stopwords]\n",
        "        return filtered_words"
      ],
      "metadata": {
        "id": "HCVR3KeLNCdB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, the TextProcessor class provides a convenient way to process text data by tokenizing it into words and cleaning those words by removing non-alphabetic words and stopwords. It is often a crucial step in text analysis and natural language processing tasks. By creating an instance of the TextProcessor class and calling its tokenize_and_clean method, you can obtain a list of cleaned and filtered words from a given input text."
      ],
      "metadata": {
        "id": "DlZYSGpROLvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ETLPipeline:\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "        self.nltk_stopwords = set(stopwords.words(\"english\"))\n",
        "\n",
        "    def run(self):\n",
        "        scraper = WebScraper(self.url)\n",
        "        article_text = scraper.extract_article_text()\n",
        "\n",
        "        processor = TextProcessor(self.nltk_stopwords)\n",
        "        filtered_words = processor.tokenize_and_clean(article_text)\n",
        "\n",
        "        word_freq = Counter(filtered_words)\n",
        "        df = pd.DataFrame(word_freq.items(), columns=[\"Words\", \"Frequencies\"])\n",
        "        df = df.sort_values(by=\"Frequencies\", ascending=False)\n",
        "        return df"
      ],
      "metadata": {
        "id": "TJLM7hC9OR5u"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, the ETLPipeline class encapsulates the end-to-end process of extracting article text from a web page, cleaning and processing the text, calculating word frequencies, and generating a sorted DataFrame. By creating an instance of the ETLPipeline class and calling its run method, you can perform the complete ETL process and obtain a DataFrame that provides insights into the most frequently used words in the article after removing stopwords."
      ],
      "metadata": {
        "id": "NeNBTwtpOkKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test and run\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    article_url = 'https://seekingalpha.com/news/4108959-ffie-gme-amc-faraday-future-intelligent-electric-ev-stock-meme-rally-markets-ibkr'\n",
        "    pipeline = ETLPipeline(article_url)\n",
        "    result_df = pipeline.run()\n",
        "    print(result_df.head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPeCnidnOkyS",
        "outputId": "252b3b58-810f-438b-af20-84541c1df49c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Words  Frequencies\n",
            "27       stock           11\n",
            "36   stockstop            8\n",
            "0      faraday            6\n",
            "35    dividend            6\n",
            "75     etfstop            5\n",
            "22      market            4\n",
            "43        news            4\n",
            "193       data            4\n",
            "2          top            4\n",
            "11       alpha            3\n",
            "167    sosnick            3\n",
            "201         ai            3\n",
            "55       value            3\n",
            "1       future            3\n",
            "9         free            3\n",
            "349     please            3\n",
            "3         nvda            3\n",
            "6      seeking            3\n",
            "21     menutop            3\n",
            "71      stocks            3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Vm5d6OVPYyQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}